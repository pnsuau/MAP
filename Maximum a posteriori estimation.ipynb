{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum A Posteriori estimation (MAP)\n",
    "Pierre-Nicolas Suau\n",
    "\n",
    "Subject: https://fr.wikipedia.org/wiki/Maximum_a_posteriori \n",
    "\n",
    "References: https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-and-maximum-a-posteriori-estimation-d7c318f9d22d <br>\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html <br>\n",
    "http://www.bdhammel.com/mle-map/ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, invgamma,beta,binom,chi2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First of all, let's get back to Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "Suppose that we have a statistical model and observations of a problem. Let's note $\\theta$ the parameter of that model and $y$ the observations. Estimating the parameter using MLE is choosing $\\theta$ in the parameter space that maximizes the likelihood function : $\\mathbb{P}(Y=y|\\Theta=\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's have a look to Maximum A Posteriori estimation (MAP).\n",
    "As previously, suppose that we have a statistical model and observations of a problem. Let's note ùúÉ the parameter of that model, ùë¶ the observations and also a distribution that quantifies ouf belief of the parameter value before taking into account any observations $\\mathbb{P}(\\Theta=\\theta)$ (it's called the prior).\n",
    "Estimating the parameter using MAP is choosing the parameter $\\theta$ that maximizes $\\mathbb{P}(\\Theta=\\theta|Y=y)$.\n",
    "\n",
    "If we use the Bayes theorem we can write:\n",
    "$$\\mathbb{P}(\\Theta=\\theta|Y=y) =\\frac{\\mathbb{P}(Y=y|\\Theta=\\theta)\\cdot\\mathbb{P}(\\Theta=\\theta)}{\\mathbb{P}(Y=y)}$$\n",
    "Given that the denominator does not depend on $\\theta$, we can only maximizes the numerator which is the likelihood multiplied by the prior.\n",
    "\n",
    "NB : Note that if we consider that we do not have any prior infomation ie $\\mathbb{P}(\\Theta=\\theta)$ is the uniform distribution, then maximizing the posterior function is equivalent to maximizing the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take an example\n",
    "\n",
    "<img src=\"kylian.jpeg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "We will consider the probability that Kylian Mbapp√© scores during the next match according his current form (number of matches with a least one goal over $N=5$ last matches). Suppose that the number of matches with a goal follows a binomial distribution with a probability $\\theta$.\n",
    "\n",
    "Lets note that Kylian scores during $k$ matches out of $n$ matches.\n",
    "If we assume that the probability of the number of matches during Kylian scores follows a binomial distribution,  it's given by :\n",
    "$$P(k\\ matches\\ with\\ a\\ goal\\ over\\ n|\\Theta=\\theta) =\\binom{n}{k} \\theta^{k} (1-\\theta)^{n-k}$$\n",
    "\n",
    "Our goal (lol) is to estimate theta. Our observed data is here the number of matches with a Kylian goal: $G=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With MLE\n",
    "Here the likelihood function is :\n",
    "$\\mathbb{P}(k=G,n=N|\\Theta=\\theta)$.\n",
    "\n",
    "The value of Œ∏ maximising the likelihood can be obtained by having derivative of the likelihood function with respect to Œ∏, and setting it to zero :\n",
    "$\\frac{dP}{d\\theta}(k=G,n=N|\\Theta=\\theta)=0$\n",
    "\n",
    "The solutions are $0$, $1$ or $\\frac{G}{N}$, because the likelihood function is defined over $[0,1]$, positive and  not the zero function, the maximum is reached for $\\theta = \\frac{G}{N}=0.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With MAP\n",
    "We all know that Kylian Mbapp√© is a really good striker, so only having $0.2$ does not look satisfying. Traducing \"being a good striker\" into mathematical terms is assuming that $\\theta$ is more likely to be close to 1 than to 0. \n",
    "For example we will take the beta distribution with $\\alpha=5$ and $\\beta=1$ as prior : $\\mathbb{P}(\\Theta=\\theta)=\\frac{\\Gamma (\\alpha + \\beta)}{\\Gamma (\\alpha)\\Gamma (\\beta)}\\theta^{\\alpha - 1} (1-\\theta)^{\\beta -1}$.\n",
    "\n",
    "Intuitively, MAP needs more information than MLE (here Mbapp√© is good striker) but performs better with few data (only last five matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to visualize the beta distribution.\n",
    "[a, b] = [5, 1]\n",
    "beta_51 = beta(a, b)\n",
    "distribution = np.linspace(0, 1) \n",
    "plt.plot(distribution, beta_51.pdf(distribution),label='Beta(5,1)') \n",
    "plt.plot(distribution, np.ones(distribution.shape),label='Uniform') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to maximize : $\\mathbb{P}(k=G,n=N|\\Theta=\\theta)\\cdot\\mathbb{P}(\\Theta=\\theta) =\\binom{n}{k} \\frac{\\Gamma (\\alpha + \\beta)}{\\Gamma (\\alpha)\\Gamma (\\beta)}\\theta^{\\alpha - 1} \\theta^{k + \\alpha -1} (1-\\theta)^{n-k + \\beta -1}$\n",
    "\n",
    "So let's compute the Kylian MAP (lol), the result is $\\frac{k + \\alpha -1}{n+\\alpha+\\beta-2}=\\frac{5}{9}=0.55$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found maximas of our functions analytically but we can do it computionally with a grid approximation. It means that we make guesses for theta (for example : [0, 0.001, 0.002 ... 1]), then compute our function (likelihood or likelihood multiplied by prior) and finally we take theta which maximizes that function as solution. It allows us to use any fonction as prior.\n",
    "\n",
    "NB : We will compute the log of the functions, so the multiplication between the likelihood and the prior become an addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mle(guesses,log_likelihood):\n",
    "    \"\"\"\n",
    "    This function returns the MLE over guesses.\n",
    "    \"\"\"\n",
    "    idx = np.argwhere(log_likelihood == log_likelihood.max())\n",
    "    return guesses[idx][0,0]\n",
    "\n",
    "\n",
    "def get_map(guesses,log_likelihood,log_prior):\n",
    "    \"\"\"\n",
    "    This function returns the MAP over guesses.\n",
    "    \"\"\"\n",
    "    log_posterior = log_likelihood + log_prior\n",
    "    idx = np.argwhere(log_posterior == log_posterior.max())\n",
    "    return guesses[idx][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations\n",
    "G=1 # One match with a goal\n",
    "N=5 # Over five matches\n",
    "\n",
    "theta_guesses = np.linspace(0.001, 1, 1000)\n",
    "\n",
    "#First we compute the log_likelihood over the guesses\n",
    "log_likelihood = np.asarray([binom.logpmf(G, N, theta) for theta in theta_guesses]) \n",
    "\n",
    "#Then we compute the MLE\n",
    "mle=get_mle(theta_guesses,log_likelihood)\n",
    "print(\"MLE\",mle)\n",
    "\n",
    "#Now we compute the log prior\n",
    "log_prior_beta_51 = [beta_51.logpdf(theta) for theta in theta_guesses]\n",
    "\n",
    "#Finally we compute the MAP with beta(5,1) prior\n",
    "map_beta_51=get_map(theta_guesses,log_likelihood,log_prior_beta_51)\n",
    "print(\"MAP with beta(5,1) prior\",map_beta_51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we found the same result.\n",
    "\n",
    "We can verify if MAP with a uniform prior returns the same result as MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prior_uniform = [np.log(1) for theta in theta_guesses]\n",
    "\n",
    "map_uniform=get_map(theta_guesses,log_likelihood,log_prior_uniform)\n",
    "print(\"MAP with uniform prior\",map_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look to the prior influence by comparing results with several functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(array):\n",
    "    return 2*(np.ones(array.shape)-array <0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution = np.linspace(0, 1) \n",
    "plt.plot(distribution, beta_51.pdf(distribution),label=\"Beta(5,1)\")\n",
    "plt.plot(distribution, np.ones(distribution.shape),label=\"Uniform\")\n",
    "plt.plot(distribution,step_function(distribution),label=\"Step function\")\n",
    "plt.plot(distribution, 2*(distribution**1),label=\"2x\")  \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prior_step = np.log(step_function(theta_guesses))\n",
    "map_beta_step=get_map(theta_guesses,log_likelihood,log_prior_step)\n",
    "print(\"MAP with step prior\",map_beta_step)\n",
    "\n",
    "log_prior_2x = [np.log(2*theta) for theta in theta_guesses]\n",
    "map_beta_2x=get_map(theta_guesses,log_likelihood,log_prior_2x)\n",
    "print(\"MAP with 2x prior\",map_beta_2x)\n",
    "\n",
    "#Previous results\n",
    "print(\"MAP with beta(5,1) prior\",map_beta_51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could have expected it intuitively, the more our prior function is high close to 1 and low close 0 the higher the MAP is. We can also notice that our step function just forbides to have theta $<0.5$.\n",
    "\n",
    "Feel free to re-run the code with other values of $(G,N)$.\n",
    "You can also test your own prior function with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_function(array):\n",
    "    \"\"\"\n",
    "    Takes an array of theta guesses as input and returns the corresponding prior values (np.array).\n",
    "    \"\"\"\n",
    "    ### To complete\n",
    "    prior=\"...\"\n",
    "    return prior\n",
    "\n",
    "plt.plot(distribution,prior_function(distribution),label=\"Prior function\")  \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "log_prior_custom = np.log(prior_function(theta_guesses))\n",
    "map_beta_custom=get_map(theta_guesses,log_likelihood,log_prior_custom)\n",
    "print(\"MAP with custom prior\",map_beta_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take another example for non football fans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"jambon.jpeg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "A strong tradition in France during \"fete du village\" is to estimate the weight of an entire raw ham, the one with the closest estimation wins the ham. The only data available is all the estimations made by other candidates (you can also take the ham in you hands but it won't be useful for our model here). We will generate estimations from other candidates with a gaussian around the real weight.\n",
    "\n",
    "So, we'll try to estimate the weight of the ham only using others' estimations. To do that, we need to make two guesses : the real weight and the error made by others (the standard deviation). Then we do the same thing as in the previous example with MLE and MAP (note that there are two prior functions, one for each guess).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood_grid(estimations,WEIGHT_GUESSES,ERROR_GUESSES):\n",
    "    log_liklelihood = [\n",
    "        [\n",
    "            norm(weight_guess, error_guess).logpdf(estimations).sum()\n",
    "            for weight_guess in WEIGHT_GUESSES\n",
    "        ]\n",
    "        for error_guess in ERROR_GUESSES\n",
    "    ]\n",
    "    return np.asarray(log_liklelihood)\n",
    "\n",
    "\n",
    "def get_mle(estimations,WEIGHT_GUESSES,ERROR_GUESSES):\n",
    "    log_likelihood = get_log_likelihood_grid(estimations,WEIGHT_GUESSES,ERROR_GUESSES)\n",
    "    idx = np.argwhere(log_likelihood == log_likelihood.max())[0][1]\n",
    "    return WEIGHT_GUESSES[idx]\n",
    "\n",
    "\n",
    "def get_map(estimations,WEIGHT_GUESSES,ERROR_GUESSES,LOG_PRIOR_GRID):\n",
    "    log_likelihood = get_log_likelihood_grid(estimations,WEIGHT_GUESSES,ERROR_GUESSES)\n",
    "    log_posterior = log_likelihood + LOG_PRIOR_GRID\n",
    "    idx = np.argwhere(log_posterior == log_posterior.max())[0][1]\n",
    "    return WEIGHT_GUESSES[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real weight \n",
    "weight=3.46 # you can try with another weight\n",
    "\n",
    "#Estimations from other candidates (in reality we wont know that)\n",
    "scale_error_estimation=0.3\n",
    "nb_canditates=10\n",
    "weight_estimations = weight + np.random.normal(loc=0, scale=scale_error_estimation,size=nb_canditates) # observations\n",
    "\n",
    "# Guesses\n",
    "weight_guesses = np.linspace(3, 4, 101)\n",
    "error_guesses = np.linspace(.1, 0.5, 100)\n",
    "\n",
    "# NOTE: Try changing the prior values and distributions\n",
    "PRIOR_WEIGHT = norm(3.500, 0.2).logpdf(weight_guesses)\n",
    "PRIOR_ERR = invgamma(5).logpdf(error_guesses)\n",
    "prior_grid = np.add.outer(PRIOR_ERR, PRIOR_WEIGHT)\n",
    "\n",
    "print(f\"The true weight of the jambon was: {weight:.3f} kg\")\n",
    "print(f\"Average measurement: {weight_estimations.mean():.3f} kg\")\n",
    "print(f\"Maximum Likelihood estimate: {get_mle(weight_estimations,weight_guesses,error_guesses):.3f} kg\")\n",
    "print(f\"Maximum A Posteriori estimate: {get_map(weight_estimations,weight_guesses,error_guesses,prior_grid):.3f} kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP and machine learning \n",
    "\n",
    "Naive Bayes methods are a set of machine learning algorithms widely used in classification for example. The probability to belong to a class is estimated with MLE or MAP (cf https://scikit-learn.org/stable/modules/naive_bayes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for running my notebook, I tried to put some jokes in a serious work to relax ourselves in such a stressful period.\n",
    "\n",
    "PS: Feel free to follow me on github, username : pnsuau (https://github.com/pnsuau)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
